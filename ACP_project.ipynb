{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.43.118:4040\n",
       "SparkContext available as 'sc' (version = 2.3.0, master = local[*], app id = local-1546162431503)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-30 10:33:41 WARN  Utils:66 - Your hostname, medard resolves to a loopback address: 127.0.1.1; using 192.168.43.118 instead (on interface wlp3s0b1)\n",
      "2018-12-30 10:33:41 WARN  Utils:66 - Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "2018-12-30 10:33:44 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.ml.feature.PCA\n",
       "import org.apache.spark.ml.linalg.Vectors\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.feature.StandardScaler\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.ml.feature.PCA\n",
    "import org.apache.spark.ml.linalg.Vectors\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = Machine_Learning_Sections/PCA/Cancer_Data\n",
       "df: org.apache.spark.sql.DataFrame = [mean radius: int, mean texture: double ... 28 more fields]\n",
       "colnames: Array[String] = Array(mean radius, mean texture, mean perimeter, mean area, mean smoothness, mean compactness, mean concavity, mean concave points, mean symmetry, mean fractal dimension, radius error, texture error, perimeter error, area error, smoothness error, compactness error, concavity error, concave points error, symmetry error, fractal dimension error, worst radius, worst texture, worst perimeter, worst area, worst smoothness, worst compactness, worst concavity, worst concave points, worst symmetry, worst fractal dimension)\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file =\"Machine_Learning_Sections/PCA/Cancer_Data\"\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(file)\n",
    "val colnames = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mean radius: integer (nullable = true)\n",
      " |-- mean texture: double (nullable = true)\n",
      " |-- mean perimeter: double (nullable = true)\n",
      " |-- mean area: double (nullable = true)\n",
      " |-- mean smoothness: double (nullable = true)\n",
      " |-- mean compactness: double (nullable = true)\n",
      " |-- mean concavity: double (nullable = true)\n",
      " |-- mean concave points: double (nullable = true)\n",
      " |-- mean symmetry: double (nullable = true)\n",
      " |-- mean fractal dimension: double (nullable = true)\n",
      " |-- radius error: double (nullable = true)\n",
      " |-- texture error: double (nullable = true)\n",
      " |-- perimeter error: double (nullable = true)\n",
      " |-- area error: double (nullable = true)\n",
      " |-- smoothness error: double (nullable = true)\n",
      " |-- compactness error: double (nullable = true)\n",
      " |-- concavity error: double (nullable = true)\n",
      " |-- concave points error: double (nullable = true)\n",
      " |-- symmetry error: double (nullable = true)\n",
      " |-- fractal dimension error: double (nullable = true)\n",
      " |-- worst radius: double (nullable = true)\n",
      " |-- worst texture: double (nullable = true)\n",
      " |-- worst perimeter: double (nullable = true)\n",
      " |-- worst area: double (nullable = true)\n",
      " |-- worst smoothness: double (nullable = true)\n",
      " |-- worst compactness: double (nullable = true)\n",
      " |-- worst concavity: double (nullable = true)\n",
      " |-- worst concave points: double (nullable = true)\n",
      " |-- worst symmetry: double (nullable = true)\n",
      " |-- worst fractal dimension: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_4689d9c1f368\n",
       "data: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(colnames).setOutputCol(\"features\")\n",
    "val data = assembler.transform(df).select($\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-30 11:27:18 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[0.0,17.99,10.38,...|\n",
      "|[1.0,20.57,17.77,...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_472d8ba0a555\n",
       "scalerModel: org.apache.spark.ml.feature.StandardScalerModel = stdScal_472d8ba0a555\n",
       "scaledData: org.apache.spark.sql.DataFrame = [features: vector]\n",
       "scaledData: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "  .setWithStd(true)\n",
    "  .setWithMean(false)\n",
    "val scalerModel = scaler.fit(data)\n",
    "var scaledData = scalerModel.transform(data).select($\"scaledFeatures\")\n",
    "scaledData = scaledData.select(scaledData(\"scaledFeatures\").as(\"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pca: org.apache.spark.ml.feature.PCAModel = pca_ce3ed9f4cbcf\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pca = (new PCA()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(3)\n",
    "  .fit(scaledData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pcaDF: org.apache.spark.sql.DataFrame = [features: vector, pcaFeatures: vector]\n",
       "result: org.apache.spark.sql.DataFrame = [pcaFeatures: vector]\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pcaDF = pca.transform(scaledData)\n",
    "val result = pcaDF.select(\"pcaFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res27: Array[org.apache.spark.sql.Row] = Array([[21.62199738236476,8.516595739466684,-3.7318474175794782]], [[15.121737034758134,2.697138979042207,-2.3546461829874357]])\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
