{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif: my task will be to try to cluster clients of a Wholesale Distributor\n",
    "based off of the sales of some product categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.clustering.KMeans\n",
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = Machine_Learning_Sections/Clustering/Wholesale customers data.csv\n",
       "df: org.apache.spark.sql.DataFrame = [Channel: int, Region: int ... 6 more fields]\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file =\"Machine_Learning_Sections/Clustering/Wholesale customers data.csv\"\n",
    "val df = spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").format(\"csv\").load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+----+-------+------+----------------+----------+\n",
      "|Channel|Region|Fresh|Milk|Grocery|Frozen|Detergents_Paper|Delicassen|\n",
      "+-------+------+-----+----+-------+------+----------------+----------+\n",
      "|      2|     3|12669|9656|   7561|   214|            2674|      1338|\n",
      "|      2|     3| 7057|9810|   9568|  1762|            3293|      1776|\n",
      "|      2|     3| 6353|8808|   7684|  2405|            3516|      7844|\n",
      "|      1|     3|13265|1196|   4221|  6404|             507|      1788|\n",
      "|      2|     3|22615|5410|   7198|  3915|            1777|      5185|\n",
      "+-------+------+-----+----+-------+------+----------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: org.apache.spark.sql.DataFrame = [Fresh: int, Milk: int ... 4 more fields]\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = df.select($\"Fresh\",$\"Milk\",$\"Grocery\",$\"Frozen\",$\"Detergents_Paper\",$\"Delicassen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Fresh: integer (nullable = true)\n",
      " |-- Milk: integer (nullable = true)\n",
      " |-- Grocery: integer (nullable = true)\n",
      " |-- Frozen: integer (nullable = true)\n",
      " |-- Detergents_Paper: integer (nullable = true)\n",
      " |-- Delicassen: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Long = 0\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// number of row contain null element \n",
    "data.filter(row => row.anyNull).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_9fa6e3711337\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val assembler = new VectorAssembler().setInputCols(Array(\"Fresh\",\n",
    "\t\t \"Milk\",\n",
    "\t\t \"Grocery\",\n",
    "\t\t \"Frozen\",\n",
    "\t\t \"Detergents_Paper\",\"Delicassen\")).setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "training_data: org.apache.spark.sql.DataFrame = [features: vector]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val training_data = assembler.transform(data).select($\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kmeans: org.apache.spark.ml.clustering.KMeans = kmeans_4c4064542ae2\n",
       "model: org.apache.spark.ml.clustering.KMeansModel = kmeans_4c4064542ae2\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val kmeans = new KMeans().setK(3).setSeed(1L)\n",
    "val model = kmeans.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [features: vector, prediction: int]\n",
       "evaluator: org.apache.spark.ml.evaluation.ClusteringEvaluator = cluEval_c1e834e01984\n",
       "silhouette: Double = 0.6660332649991575\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make predictions\n",
    "val predictions = model.transform(training_data)\n",
    "// Evaluate clustering by computing Silhouette score\n",
    "val evaluator = new ClusteringEvaluator()\n",
    "val silhouette = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6660332649991575\n",
      "Cluster Centers: \n",
      "[7993.574780058651,4196.803519061584,5837.4926686217,2546.624633431085,2016.2873900293255,1151.4193548387098]\n",
      "[9928.18918918919,21513.081081081084,30993.486486486487,2960.4324324324325,13996.594594594595,3772.3243243243246]\n",
      "[35273.854838709674,5213.919354838709,5826.096774193548,6027.6612903225805,1006.9193548387096,2237.6290322580644]\n"
     ]
    }
   ],
   "source": [
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")\n",
    "// Shows the result.\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
